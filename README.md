# HILRecognizer

# Abstract

Recognizing entities that follow or closely resemble a regular expression (regex) pattern is an important task in information extraction. Common approaches for extraction of such entities require humans to either write a regex recognizing an entity or manually label entity mentions in a document corpus. In this project, we propose an iterative human-in-the-loop (HIL) framework that allows users to write a regex or manually label entity mentions, followed by training and refining a classifier based on the provided information.

# Get Started

HILRecognizer contains code and datasets for our KDD 2019 publication:

[**How to Invest my Time: Lessons from Human-in-the-Loop Entity Extraction**](https://dl.acm.org/doi/abs/10.1145/3292500.3330773?casa_token=8BqHmlAfMVUAAAAA:nvpeahAQ4wzjSd93Tt3IYggimKpjW9EDXRjokQsVY2MMA6Y97i_9DOnP1rK85lEdAD7am1BAuduz)

To run the code, the following environment is required:
* python==2.7.6
* torch==0.3.1

# Run 5 fold cross validation. 
The 5-fold cross validation is used to select the best hyperparameters based on the weaklly labeled data, via ``random search`` technique. 
After the 5 fold cross validation, the best hyperparameter ``XX.pkl`` is output to the ``outfolder`` folder.

``
outfolder="experiments/position/kaggle_bound/pretrain"
CUDA_VISIBLE_DEVICES="$dev" 
``

``
python s_train_bilstm_tagger.py --data data/testDateTimeAll.csv 
--save "$outfolder" --pooling all --partition loo --epochs 5 --cuda --batch-size 512 
--tags o y --max_len 104 --label R.E.tag.gr6
``


# Pretrain on weakly labeled data

Pretrain a deep model based on the weakly labeled data. Weak labels are generated by Regular Expressions.

``CUDA_VISIBLE_DEVICES="$dev" ``

``
python s_train_bilstm_tagger.py --data data/testDateTimeAll.csv 
--save experiments/position/kaggle_bound/pretrain 
--params experiments/position/kaggle_bound/loo_R.E.tag_best_args.pkl 
--epochs 5 --cuda --batch-size 512 --tags o y --max_len 104 --label R.E.tag --run pretrain
``

# Fine-tuninig pre-trained model with active learning

Fine-tune the pre-trained model with ``albs`` human labels each iteration.

aliter: active learning iterations \
albs: active learning batch size \
epoch: active learning epochs \
pretrain: pretrained model

``
aliter=50  albs=20  epoch=10 
best_args="loo_R.E.tag_best_args.pkl" 
pretrain= "R.E.tag_testKaggleAll_pretrain_5.pt" 
outfolder="active_learning_cv_by_outlet_retag_pt5"
``

``
CUDA_VISIBLE_DEVICES="$dev" 
``

``
python s_train_bilstm_tagger.py --data data/testDateTimeAll.csv 
--save experiments/position/kaggle_bound/"$outfolder"/ 
--params experiments/position/kaggle_bound/"$best_args" 
--pretrain experiments/position/kaggle_bound/pretrain/"$pretrain" 
--epochs 10 --cuda --partition outlet --batch-size 300 --tags o y --max_len 104 --label TagLabel 
--fold "$fold" --run al --al_bs "$albs" --al_iter "$aliter" 
``
